{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops>=0.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: torch>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: datasets>=2.14.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.25.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.25.2)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.33.0)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.66.1)\n",
      "Requirement already satisfied: pandas>=2.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: wandb>=0.15.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.15.9)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.2.21)\n",
      "Requirement already satisfied: rich>=13.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (13.5.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.22.0)\n",
      "Requirement already satisfied: timeout-decorator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.5.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (0.16.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.14.4->-r requirements.txt (line 4)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 6)) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 6)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 6)) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=2.0.3->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=2.0.3->-r requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=2.0.3->-r requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (8.1.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (3.1.34)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (1.30.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb>=0.15.8->-r requirements.txt (line 9)) (4.23.4)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jaxtyping->-r requirements.txt (line 11)) (4.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=13.5.2->-r requirements.txt (line 12)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=13.5.2->-r requirements.txt (line 12)) (2.15.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 13)) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 13)) (3.2.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.15.8->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.4->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.15.8->-r requirements.txt (line 9)) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->-r requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->datasets>=2.14.4->-r requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.4->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.4->-r requirements.txt (line 4)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.14.4->-r requirements.txt (line 4)) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.15.8->-r requirements.txt (line 9)) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import wandb\n",
    "import pickle\n",
    "import json\n",
    "import types\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "import itertools\n",
    "import warnings\n",
    "import loralib as lora\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from gpt2_lora.data_utils import FT_Dataset\n",
    "from gpt2_lora.model import GPT2LMModel, GPT2Config, LORAConfig\n",
    "from gpt2_lora.training.train import train_validate\n",
    "from gpt2_lora.correction_dataset import CorrectionDataset, create_lm_dataset \n",
    "import gpt2_lora.ablations as ablations\n",
    "import gpt2_lora.activation_graft as activation_grafts\n",
    "from gpt2_lora.training.optimizer import (\n",
    "    create_optimizer_scheduler, \n",
    "    add_optimizer_params, \n",
    "    create_adam_optimizer_from_args\n",
    ")\n",
    "from gpt2_lora.exp_utils import create_exp_dir\n",
    "from gpt2_lora.training.evaluate import evaluate\n",
    "from gpt2_lora.utils import set_all_trainable, set_trainable_from_graft, AverageMeter, log_experiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lora_compensation.hooking import get_residuals_and_logits\n",
    "from timeout_decorator import timeout, TimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    if args.rank == 0:\n",
    "        print('=' * 100)\n",
    "        for k, v in args.__dict__.items():\n",
    "            print(f'        - {k} : {v}')\n",
    "        print('=' * 100)\n",
    "        \n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def validate_args(args):\n",
    "    if args.task not in ['lora_graft_finetune', 'lora_mlp_finetune', 'lora_attn_finetune', 'lora_all_finetune', 'finetune', 'graft_finetune']: \n",
    "        raise ValueError(\"task not recognized\")\n",
    "    if args.task==\"lora_graft_finetune\": \n",
    "        if sum([args.adapt_mlp_c_fc, args.adapt_mlp_c_proj, args.adapt_attn_c_attn, args.adapt_attn_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA layers selected\")\n",
    "    if args.task==\"lora_mlp_finetune\": \n",
    "        if sum([args.adapt_mlp_c_fc, args.adapt_mlp_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA MLP layers selected\")\n",
    "    if args.task==\"lora_attn_finetune\": \n",
    "        if sum([args.aadapt_attn_c_attn, args.adapt_attn_c_proj]) == 0: \n",
    "            raise ValueError(\"No LoRA Attention layers selected\")\n",
    "    if args.graft_type not in [\"decomposition\", \"causal_total_effect\", \"causal_total_effect_window\", \"causal_direct_effect_window\"]: \n",
    "        raise ValueError(\"graft_type not recognized\")\n",
    "    if args.ablation_method not in [\"noise\", \"resample\", \"resample_uniform\"]: \n",
    "        raise ValueError(\"ablation_method not recognized\")\n",
    "    \n",
    "\n",
    "def parse_args(config_path=\"configs/config_lora_compensatory.yaml\"):\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "\n",
    "    args = types.SimpleNamespace()\n",
    "    \n",
    "    for key, value in config.items():\n",
    "        setattr(args, key, value)\n",
    "\n",
    "    setattr(args, 'device', get_device())      \n",
    "    validate_args(args)\n",
    "    print_args(args)\n",
    "    \n",
    "    return args\n",
    "\n",
    "def generate_lora_configs(layer: int, n_layers: int, args : types.SimpleNamespace, adapt_attn=True):\n",
    "    lora_configs = [\n",
    "        {\"attn\" : None, \"mlp\" : None} for _ in range(n_layers)\n",
    "    ]\n",
    "    if layer is None:\n",
    "        return lora_configs\n",
    "    if adapt_attn:\n",
    "        lora_configs[layer][\"attn\"] = LORAConfig(\n",
    "                        layer=layer,\n",
    "                        layer_type=\"attn\",\n",
    "                        adapt_attn_c_attn=args.adapt_attn_c_attn,\n",
    "                        adapt_attn_c_proj=args.adapt_attn_c_proj,\n",
    "                        adapt_mlp_c_fc=False,\n",
    "                        adapt_mlp_c_proj=False,\n",
    "                        lora_dim=args.lora_dim,\n",
    "                        lora_alpha=args.lora_alpha,\n",
    "                        lora_dropout=args.lora_dropout)\n",
    "    else: \n",
    "        lora_configs[layer][\"mlp\"] = LORAConfig(\n",
    "                        layer=layer,\n",
    "                        layer_type=\"mlp\",\n",
    "                        adapt_attn_c_attn=False,\n",
    "                        adapt_attn_c_proj=False,\n",
    "                        adapt_mlp_c_fc=args.adapt_mlp_c_fc,\n",
    "                        adapt_mlp_c_proj=args.adapt_mlp_c_proj,\n",
    "                        lora_dim=args.lora_dim,\n",
    "                        lora_alpha=args.lora_alpha,\n",
    "                        lora_dropout=args.lora_dropout) \n",
    "    return lora_configs\n",
    "\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_control(args): \n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    if args.model_name == \"gpt2-small\":\n",
    "            hf_model_name = \"gpt2\"\n",
    "            n_layer = 12\n",
    "            config = GPT2Config(\n",
    "                n_embd=768, n_layer=n_layer, n_head=12, \n",
    "    )\n",
    "    elif args.model_name == \"gpt2-large\":\n",
    "            hf_model_name = args.model_name\n",
    "            n_layer = 36\n",
    "            config = GPT2Config(\n",
    "                n_embd=1280, n_layer=n_layer, n_head=20, \n",
    "    )\n",
    "    else: \n",
    "        raise ValueError(\"model_name not recognized\")\n",
    "    lora_configs = generate_lora_configs(None, n_layer, args)\n",
    "\n",
    "    lm_net = GPT2LMModel(config, lora_configs)\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "    state_dict = model.state_dict()\n",
    "    lm_net.load_weight(state_dict)   \n",
    "    \n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        args.model_name, \n",
    "        hf_model=lm_net,\n",
    "        lora_case=True\n",
    "    )\n",
    "    correction_dataset = CorrectionDataset(args.fact_data)\n",
    "\n",
    "    #start by getting the base results\n",
    "    correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "    initial_results = []\n",
    "    print(\"..getting initial results\")\n",
    "    for batch_idx, batch in enumerate(correction_dataloader):\n",
    "        #----------------------------Prepare Correction Dataset-----------------------------#\n",
    "        prompt = batch[\"prompt\"][0]\n",
    "        subject = batch[\"subject\"][0]\n",
    "        target = batch[\"target\"][0]\n",
    "        target_new = batch[\"target_new\"][0]\n",
    "        \n",
    "        @timeout(30)\n",
    "        def timeout_resample(ablation_method):\n",
    "            if ablation_method == \"resample_uniform\": \n",
    "                original_fact, corrupted_facts, _ = ablations.resample_ablation_uniform(model, prompt,subject,target,                                                             n_noise_samples=args.noise_samples)\n",
    "            elif ablation_method==\"resample\":\n",
    "                original_fact, corrupted_facts, _ = ablations.resample_ablation(model, prompt, subject, target, n_noise_samples=args.noise_samples, temperature=args.temperature)\n",
    "            elif ablation_method==\"noise\": \n",
    "                original_fact, corrupted_facts, _ = ablations.noise_ablation(model, prompt,subject,target,n_noise_samples=args.noise_samples)\n",
    "            else: \n",
    "                raise ValueError(\"ablation_method not recognized\")\n",
    "            return original_fact, corrupted_facts\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            original_fact, corrupted_facts = timeout_resample(args.ablation_method)\n",
    "        except TimeoutError:\n",
    "            warnings.warn(f\"Resample timed out for prompt {prompt}\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        res = get_residuals_and_logits(model, \n",
    "                                args.device,\n",
    "                                clean_prompt=original_fact,\n",
    "                                corrupted_prompts=corrupted_facts,\n",
    "                                target=target, \n",
    "                                target_new=target_new, \n",
    "                                ablate_with_corrupted=True)\n",
    "        initial_results.append(res)\n",
    "    save_pickle(initial_results, f\"lora_compensation/results/{args.experiment_name}_initial_results_large.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "        - experiment_name : lora_compensatory_adapt_all\n",
      "        - fact_data : datasets/chatgpt_fact_dataset_100.json\n",
      "        - task : lora_graft_finetune\n",
      "        - random_seed : 1\n",
      "        - model_name : gpt2-large\n",
      "        - use_hf_model : True\n",
      "        - ablation_method : resample_uniform\n",
      "        - use_mle_token_graft : False\n",
      "        - graft_type : decomposition\n",
      "        - noise_samples : 10\n",
      "        - graft_threshold : 0.75\n",
      "        - temperature : 0.85\n",
      "        - window_size : 5\n",
      "        - window_stride : 1\n",
      "        - do_wandb : False\n",
      "        - log_interval : 10\n",
      "        - eval_interval : 10\n",
      "        - save_interval : 500\n",
      "        - lora_dim : 2\n",
      "        - lora_alpha : 128\n",
      "        - lora_dropout : 0.2\n",
      "        - adapt_mlp_c_fc : True\n",
      "        - adapt_mlp_c_proj : True\n",
      "        - adapt_attn_c_attn : True\n",
      "        - adapt_attn_c_proj : True\n",
      "        - test_size : 0.1\n",
      "        - completion_size : 0.2\n",
      "        - train_batch_size : 16\n",
      "        - valid_batch_size : 4\n",
      "        - grad_acc : 1\n",
      "        - seq_len : 32\n",
      "        - max_epoch : 50\n",
      "        - lr : 0.0001\n",
      "        - weight_decay : 0.01\n",
      "        - correct_bias : True\n",
      "        - adam_epislon : 1e-6\n",
      "        - no_decay_bias : True\n",
      "        - adam_beta1 : 0.9\n",
      "        - adam_beta2 : 0.98\n",
      "        - scheduler : linear\n",
      "        - max_step : None\n",
      "        - warmup_step : 0\n",
      "        - i_steps : 0\n",
      "        - i_lrs : 0.00025\n",
      "        - init_checkpoint : None\n",
      "        - fp16 : False\n",
      "        - work_dir : None\n",
      "        - obj : clm\n",
      "        - label_smooth : 0.0\n",
      "        - roll_interval : -1\n",
      "        - roll_lr : 0.0001\n",
      "        - roll_step : 100\n",
      "        - eval_epoch : 1\n",
      "        - clip : 1.0\n",
      "        - rank : 0\n",
      "        - save_model : False\n",
      "        - device : cuda\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064a6c44d7074eb09681ad8edf96222c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf636c9146e47dfa2761d2c82a1bb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8f7873fe79474e8ff4d3ebbdcda981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad6b81771cd4381af147745c7f5bbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2115bab56ad245b884a9ff7858ff1f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f52688d3e04f9ab6a853501bd2d662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n",
      "..getting initial results\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "if args.do_wandb: \n",
    "    wandb.login()\n",
    "    \n",
    "random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)   \n",
    "\n",
    "run_control(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(args, start_layer=0): \n",
    "    \n",
    "\n",
    "    if args.model_name == \"gpt2-small\":\n",
    "            hf_model_name = \"gpt2\"\n",
    "            n_layer = 12\n",
    "            config = GPT2Config(\n",
    "                n_embd=768, n_layer=n_layer, n_head=12, \n",
    "    )\n",
    "    elif args.model_name == \"gpt2-large\":\n",
    "            hf_model_name = args.model_name\n",
    "            n_layer = 36\n",
    "            config = GPT2Config(\n",
    "                n_embd=1280, n_layer=n_layer, n_head=20, \n",
    "    )\n",
    "    else: \n",
    "        raise ValueError(\"model_name not recognized\")\n",
    "    \n",
    "    correction_dataset = CorrectionDataset(args.fact_data)\n",
    "    \n",
    "    #start by getting the base results\n",
    "    correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "    results = []        \n",
    "\n",
    "    for layer in range(start_layer, n_layer*2):\n",
    "        torch.set_grad_enabled(True)\n",
    "        if layer % 2 == 0:\n",
    "            adapt_attention = True\n",
    "            layer = layer // 2\n",
    "        else: \n",
    "            adapt_attention = False\n",
    "            layer = layer // 2\n",
    "          \n",
    "        x = \"attn\" if adapt_attention else \"mlp\"\n",
    "        print(f\"running layer {layer}, block {layer //2} {x}\")\n",
    "            \n",
    "        lora_configs = generate_lora_configs(layer, n_layer, args, adapt_attn=adapt_attention)\n",
    "        print(\"init model\")\n",
    "        lm_net = GPT2LMModel(config, lora_configs)\n",
    "        model = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(hf_model_name)\n",
    "        state_dict = model.state_dict()\n",
    "        lm_net.load_weight(state_dict)  \n",
    "        \n",
    "        lora.mark_only_lora_as_trainable(lm_net)\n",
    "        \n",
    "        \n",
    "        correction_dataset = CorrectionDataset(args.fact_data)\n",
    "        correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "        \n",
    "        all_dataset = [] ; all_dataset_ref = []\n",
    "        for batch_idx, batch in enumerate(correction_dataloader):\n",
    "            #----------------------------Prepare Correction Dataset-----------------------------#\n",
    "            prompt = batch[\"prompt\"][0]\n",
    "            subject = batch[\"subject\"][0]\n",
    "            target = batch[\"target\"][0]\n",
    "            target_new = batch[\"target_new\"][0]\n",
    "            training_prompts = [p[0] for p in batch[\"training_prompts\"]]\n",
    "            reference_evaluation_prompts = [p[0] for p in batch[\"reference_evaluation_prompts\"]]\n",
    "\n",
    "            dataset = create_lm_dataset(\n",
    "                    prompts=training_prompts, target=target_new,\n",
    "                    subject=subject, tokenizer=tokenizer, args=args\n",
    "                )\n",
    "            dataset_ref = create_lm_dataset(\n",
    "                prompts=reference_evaluation_prompts, target=target,\n",
    "                subject=subject, tokenizer=tokenizer, args=args\n",
    "            )\n",
    "            all_dataset += dataset\n",
    "            all_dataset_ref += dataset_ref\n",
    "            \n",
    "        dataset_indices = list(range(len(dataset)))\n",
    "        training_indices, valid_indices = train_test_split(\n",
    "            dataset_indices, test_size=args.test_size, random_state=args.random_seed\n",
    "        )\n",
    "        training_prompts = [d for i,d in enumerate(all_dataset) if i in training_indices]\n",
    "        valid_prompts = [d for i,d in enumerate(all_dataset) if i in valid_indices]\n",
    "        training_prompts_ref = [d for i,d in enumerate(all_dataset_ref) if i in training_indices]\n",
    "        valid_prompts_ref = [d for i,d in enumerate(all_dataset_ref) if i in valid_indices]\n",
    "        \n",
    "\n",
    "        train_data = FT_Dataset(\n",
    "            samples=training_prompts,\n",
    "            ref_samples=training_prompts_ref,\n",
    "            batch_size=args.train_batch_size,\n",
    "            max_seq_length=args.seq_len, \n",
    "            joint_lm=args.obj=='jlm'\n",
    "        ) \n",
    "        valid_data = FT_Dataset(\n",
    "            samples=valid_prompts,\n",
    "            ref_samples=valid_prompts_ref,\n",
    "            batch_size=args.train_batch_size,\n",
    "            max_seq_length=args.seq_len, \n",
    "            joint_lm=args.obj=='jlm'\n",
    "        )     \n",
    "        train_loader = DataLoader(\n",
    "            train_data, batch_size=args.train_batch_size, num_workers=0, \n",
    "            shuffle=False, pin_memory=False, drop_last=True,\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_data, batch_size=args.valid_batch_size, num_workers=0, \n",
    "            shuffle=False, pin_memory=False, drop_last=False,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #---------------------------------Training Model------------------------------------#\n",
    "        \n",
    "        optimizer = create_adam_optimizer_from_args(lm_net, args)\n",
    "        if args.device=='cuda':\n",
    "            print('using cuda.')\n",
    "            lm_net = lm_net.cuda()\n",
    "            \n",
    "        print(\"..training\")\n",
    "        if args.fp16:\n",
    "            try:\n",
    "                from torch.cuda import amp\n",
    "            except Exception as e:\n",
    "                warnings.warn('Could not import amp, apex may not be installed')\n",
    "        if args.max_step is None:\n",
    "            args.max_step = (args.max_epoch * train_data.num_batches) \n",
    "            print('set max_step:', args.max_step)\n",
    "        scheduler = create_optimizer_scheduler(optimizer, args)\n",
    "        if args.fp16:\n",
    "            lm_net, optimizer = amp.initialize(lm_net, optimizer, opt_level=\"O1\")\n",
    "        try:\n",
    "            train_step = 0\n",
    "            for epoch in itertools.count(start=1):\n",
    "                train_step = train_validate(\n",
    "                    lm_net, optimizer, scheduler, train_loader, valid_loader, args, \n",
    "                    train_step=train_step, epoch=epoch\n",
    "                )                \n",
    "                if train_step >= args.max_step or (args.max_epoch is not None and epoch >= args.max_epoch):\n",
    "                    if args.rank == 0:\n",
    "                        print('-' * 100)\n",
    "                        print('End of training')\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            if args.rank == 0:\n",
    "                print('-' * 100)\n",
    "                print('Exiting from training early')\n",
    "            early_exit = True\n",
    "\n",
    "    \n",
    "        #--------------------Hooking model------------------\n",
    "        torch.set_grad_enabled(False)\n",
    "        lm_net = lm_net.to(\"cpu\") #temporary move to cpu to load onto hooked transformer\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            args.model_name, \n",
    "            hf_model=lm_net,\n",
    "            lora_case=True,\n",
    "            device = args.device\n",
    "        )\n",
    "        del lm_net\n",
    "        del optimizer\n",
    "        del scheduler\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "        correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "        results = []\n",
    "        for batch_idx, batch in enumerate(correction_dataloader):\n",
    "            #----------------------------Prepare Correction Dataset-----------------------------#\n",
    "            prompt = batch[\"prompt\"][0]\n",
    "            subject = batch[\"subject\"][0]\n",
    "            target = batch[\"target\"][0]\n",
    "            target_new = batch[\"target_new\"][0]\n",
    "            training_prompts = [p[0] for p in batch[\"training_prompts\"]]\n",
    "\n",
    "            @timeout(30)\n",
    "            def timeout_resample(ablation_method):\n",
    "                if ablation_method == \"resample_uniform\": \n",
    "                    original_fact, corrupted_facts, _ = ablations.resample_ablation_uniform(model, prompt,subject,target,                                                             n_noise_samples=args.noise_samples)\n",
    "                elif ablation_method==\"resample\":\n",
    "                    original_fact, corrupted_facts, _ = ablations.resample_ablation(model, prompt, subject, target, n_noise_samples=args.noise_samples, temperature=args.temperature)\n",
    "                elif ablation_method==\"noise\": \n",
    "                    original_fact, corrupted_facts, _ = ablations.noise_ablation(model, prompt,subject,target,n_noise_samples=args.noise_samples)\n",
    "                else: \n",
    "                    raise ValueError(\"ablation_method not recognized\")\n",
    "                return original_fact, corrupted_facts\n",
    "\n",
    "            try:\n",
    "                original_fact, corrupted_facts = timeout_resample(args.ablation_method)\n",
    "            except TimeoutError:\n",
    "                warnings.warn(f\"Resample timed out for prompt {prompt}\")\n",
    "                continue\n",
    "\n",
    "            with torch.no_grad():\n",
    "                res = get_residuals_and_logits(model, \n",
    "                                        args.device,\n",
    "                                        clean_prompt=original_fact,\n",
    "                                        corrupted_prompts=corrupted_facts,\n",
    "                                        target=target, \n",
    "                                        target_new=target_new, \n",
    "                                        ablate_with_corrupted=True)\n",
    "            results.append(res)\n",
    "            module = \"attn\" if adapt_attention else \"mlp\"\n",
    "        save_pickle(results, f\"lora_compensation/results/{args.experiment_name}_layer_{layer}_{module}_results_large.pkl\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "        - experiment_name : lora_compensatory_adapt_all\n",
      "        - fact_data : datasets/chatgpt_fact_dataset_100.json\n",
      "        - task : lora_graft_finetune\n",
      "        - random_seed : 1\n",
      "        - model_name : gpt2-large\n",
      "        - use_hf_model : True\n",
      "        - ablation_method : resample_uniform\n",
      "        - use_mle_token_graft : False\n",
      "        - graft_type : decomposition\n",
      "        - noise_samples : 10\n",
      "        - graft_threshold : 0.75\n",
      "        - temperature : 0.85\n",
      "        - window_size : 5\n",
      "        - window_stride : 1\n",
      "        - do_wandb : False\n",
      "        - log_interval : 10\n",
      "        - eval_interval : 10\n",
      "        - save_interval : 500\n",
      "        - lora_dim : 2\n",
      "        - lora_alpha : 128\n",
      "        - lora_dropout : 0.2\n",
      "        - adapt_mlp_c_fc : True\n",
      "        - adapt_mlp_c_proj : True\n",
      "        - adapt_attn_c_attn : True\n",
      "        - adapt_attn_c_proj : True\n",
      "        - test_size : 0.1\n",
      "        - completion_size : 0.2\n",
      "        - train_batch_size : 16\n",
      "        - valid_batch_size : 4\n",
      "        - grad_acc : 1\n",
      "        - seq_len : 32\n",
      "        - max_epoch : 50\n",
      "        - lr : 0.0001\n",
      "        - weight_decay : 0.01\n",
      "        - correct_bias : True\n",
      "        - adam_epislon : 1e-6\n",
      "        - no_decay_bias : True\n",
      "        - adam_beta1 : 0.9\n",
      "        - adam_beta2 : 0.98\n",
      "        - scheduler : linear\n",
      "        - max_step : None\n",
      "        - warmup_step : 0\n",
      "        - i_steps : 0\n",
      "        - i_lrs : 0.00025\n",
      "        - init_checkpoint : None\n",
      "        - fp16 : False\n",
      "        - work_dir : None\n",
      "        - obj : clm\n",
      "        - label_smooth : 0.0\n",
      "        - roll_interval : -1\n",
      "        - roll_lr : 0.0001\n",
      "        - roll_step : 100\n",
      "        - eval_epoch : 1\n",
      "        - clip : 1.0\n",
      "        - rank : 0\n",
      "        - save_model : False\n",
      "        - device : cuda\n",
      "====================================================================================================\n",
      "running layer 0, block 0 attn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/LoRA/gpt2_lora/correction_dataset.py:23: UserWarning: The subject or target does not seem to be in the prompt.\n",
      "  warnings.warn(\"The subject or target does not seem to be in the prompt.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda.\n",
      "..training\n",
      "set max_step: 350\n",
      "start to train the model................ 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/ec2-user/SageMaker/LoRA/gpt2_lora/training/optimizer.py:136: UserWarning: This overload of addcdiv_ is deprecated:\n",
      "\taddcdiv_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcdiv_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /opt/conda/conda-bld/pytorch_1686274778240/work/torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
      "  p.data.addcdiv_(-step_size, exp_avg, denom)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to train the model................ 2\n",
      "| epoch   2 step       10 |      4 batches | lr 9.71e-05 | ms/batch 175.31 | loss  9.55 | avg loss  9.74 | ppl 16925.97\n",
      "average loss 8.843397776285807\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.40s | valid loss  8.84 | valid ppl 6928.49 | best ppl 6928.49 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "| epoch   4 step       20 |      2 batches | lr 9.43e-05 | ms/batch 88.08 | loss  8.33 | avg loss  8.05 | ppl 3118.21\n",
      "average loss 7.375507990519206\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step       20 | time:  0.40s | valid loss  7.38 | valid ppl 1596.40 | best ppl 1596.40 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 5\n",
      "| epoch   5 step       30 |      6 batches | lr 9.14e-05 | ms/batch 265.52 | loss  7.15 | avg loss  7.56 | ppl 1912.08\n",
      "average loss 6.445512930552165\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step       30 | time:  0.40s | valid loss  6.45 | valid ppl 629.87 | best ppl 629.87 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 6\n",
      "start to train the model................ 7\n",
      "| epoch   7 step       40 |      4 batches | lr 8.86e-05 | ms/batch 177.59 | loss  6.97 | avg loss  6.79 | ppl 887.70\n",
      "average loss 5.68599001566569\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step       40 | time:  0.40s | valid loss  5.69 | valid ppl 294.71 | best ppl 294.71 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 8\n",
      "start to train the model................ 9\n",
      "| epoch   9 step       50 |      2 batches | lr 8.57e-05 | ms/batch 88.76 | loss  6.41 | avg loss  5.99 | ppl 398.65\n",
      "average loss 5.052548249562581\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step       50 | time:  0.40s | valid loss  5.05 | valid ppl 156.42 | best ppl 156.42 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 10\n",
      "| epoch  10 step       60 |      6 batches | lr 8.29e-05 | ms/batch 268.74 | loss  5.31 | avg loss  5.70 | ppl 299.90\n",
      "average loss 4.46506388982137\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step       60 | time:  0.41s | valid loss  4.47 | valid ppl 86.93 | best ppl 86.93 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 11\n",
      "start to train the model................ 12\n",
      "| epoch  12 step       70 |      4 batches | lr 8e-05 | ms/batch 180.06 | loss  5.41 | avg loss  5.23 | ppl 186.20\n",
      "average loss 3.958106517791748\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   7 at step       70 | time:  0.40s | valid loss  3.96 | valid ppl 52.36 | best ppl 52.36 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 13\n",
      "start to train the model................ 14\n",
      "| epoch  14 step       80 |      2 batches | lr 7.71e-05 | ms/batch 89.88 | loss  5.19 | avg loss  4.72 | ppl 112.26\n",
      "average loss 3.5423900286356607\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   8 at step       80 | time:  0.41s | valid loss  3.54 | valid ppl 34.55 | best ppl 34.55 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 15\n",
      "| epoch  15 step       90 |      6 batches | lr 7.43e-05 | ms/batch 272.53 | loss  4.04 | avg loss  4.47 | ppl 87.54\n",
      "average loss 3.22083846728007\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   9 at step       90 | time:  0.41s | valid loss  3.22 | valid ppl 25.05 | best ppl 25.05 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 16\n",
      "start to train the model................ 17\n",
      "| epoch  17 step      100 |      4 batches | lr 7.14e-05 | ms/batch 182.08 | loss  4.37 | avg loss  4.25 | ppl 70.41\n",
      "average loss 2.9791585206985474\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  10 at step      100 | time:  0.41s | valid loss  2.98 | valid ppl 19.67 | best ppl 19.67 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 18\n",
      "start to train the model................ 19\n",
      "| epoch  19 step      110 |      2 batches | lr 6.86e-05 | ms/batch 90.71 | loss  4.51 | avg loss  4.01 | ppl 55.35\n",
      "average loss 2.7745598355929055\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  11 at step      110 | time:  0.41s | valid loss  2.77 | valid ppl 16.03 | best ppl 16.03 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 20\n",
      "| epoch  20 step      120 |      6 batches | lr 6.57e-05 | ms/batch 272.73 | loss  3.34 | avg loss  3.74 | ppl 42.03\n",
      "average loss 2.617851138114929\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  12 at step      120 | time:  0.41s | valid loss  2.62 | valid ppl 13.71 | best ppl 13.71 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 21\n",
      "start to train the model................ 22\n",
      "| epoch  22 step      130 |      4 batches | lr 6.29e-05 | ms/batch 180.73 | loss  3.76 | avg loss  3.67 | ppl 39.06\n",
      "average loss 2.469179630279541\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  13 at step      130 | time:  0.41s | valid loss  2.47 | valid ppl 11.81 | best ppl 11.81 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 23\n",
      "start to train the model................ 24\n",
      "| epoch  24 step      140 |      2 batches | lr 6e-05 | ms/batch 90.23 | loss  4.08 | avg loss  3.61 | ppl 36.98\n",
      "average loss 2.355377415815989\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  14 at step      140 | time:  0.40s | valid loss  2.36 | valid ppl 10.54 | best ppl 10.54 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 25\n",
      "| epoch  25 step      150 |      6 batches | lr 5.71e-05 | ms/batch 269.37 | loss  2.97 | avg loss  3.31 | ppl 27.35\n",
      "average loss 2.2658329208691916\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  15 at step      150 | time:  0.41s | valid loss  2.27 | valid ppl  9.64 | best ppl  9.64 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 26\n",
      "start to train the model................ 27\n",
      "| epoch  27 step      160 |      4 batches | lr 5.43e-05 | ms/batch 178.88 | loss  3.32 | avg loss  3.32 | ppl 27.54\n",
      "average loss 2.1826584736506143\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  16 at step      160 | time:  0.41s | valid loss  2.18 | valid ppl  8.87 | best ppl  8.87 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 28\n",
      "start to train the model................ 29\n",
      "| epoch  29 step      170 |      2 batches | lr 5.14e-05 | ms/batch 88.86 | loss  3.78 | avg loss  3.35 | ppl 28.55\n",
      "average loss 2.1155537565549216\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  17 at step      170 | time:  0.40s | valid loss  2.12 | valid ppl  8.29 | best ppl  8.29 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 30\n",
      "| epoch  30 step      180 |      6 batches | lr 4.86e-05 | ms/batch 268.80 | loss  2.66 | avg loss  3.02 | ppl 20.55\n",
      "average loss 2.055179556210836\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  18 at step      180 | time:  0.40s | valid loss  2.06 | valid ppl  7.81 | best ppl  7.81 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 31\n",
      "start to train the model................ 32\n",
      "| epoch  32 step      190 |      4 batches | lr 4.57e-05 | ms/batch 178.10 | loss  3.11 | avg loss  3.09 | ppl 22.00\n",
      "average loss 2.0135168631871543\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  19 at step      190 | time:  0.40s | valid loss  2.01 | valid ppl  7.49 | best ppl  7.49 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 33\n",
      "start to train the model................ 34\n",
      "| epoch  34 step      200 |      2 batches | lr 4.29e-05 | ms/batch 88.80 | loss  3.61 | avg loss  3.15 | ppl 23.27\n",
      "average loss 1.9740041891733806\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  20 at step      200 | time:  0.40s | valid loss  1.97 | valid ppl  7.20 | best ppl  7.20 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 35\n",
      "| epoch  35 step      210 |      6 batches | lr 4e-05 | ms/batch 267.30 | loss  2.48 | avg loss  2.84 | ppl 17.17\n",
      "average loss 1.9282054901123047\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  21 at step      210 | time:  0.40s | valid loss  1.93 | valid ppl  6.88 | best ppl  6.88 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 36\n",
      "start to train the model................ 37\n",
      "| epoch  37 step      220 |      4 batches | lr 3.71e-05 | ms/batch 177.83 | loss  2.94 | avg loss  2.92 | ppl 18.51\n",
      "average loss 1.8913475473721821\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  22 at step      220 | time:  0.40s | valid loss  1.89 | valid ppl  6.63 | best ppl  6.63 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 38\n",
      "start to train the model................ 39\n",
      "| epoch  39 step      230 |      2 batches | lr 3.43e-05 | ms/batch 88.87 | loss  3.50 | avg loss  3.03 | ppl 20.67\n",
      "average loss 1.8665404319763184\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  23 at step      230 | time:  0.40s | valid loss  1.87 | valid ppl  6.47 | best ppl  6.47 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 40\n",
      "| epoch  40 step      240 |      6 batches | lr 3.14e-05 | ms/batch 266.57 | loss  2.38 | avg loss  2.73 | ppl 15.33\n",
      "average loss 1.8333277702331543\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  24 at step      240 | time:  0.40s | valid loss  1.83 | valid ppl  6.25 | best ppl  6.25 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 41\n",
      "start to train the model................ 42\n",
      "| epoch  42 step      250 |      4 batches | lr 2.86e-05 | ms/batch 177.67 | loss  2.83 | avg loss  2.83 | ppl 17.01\n",
      "average loss 1.8122981985410054\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  25 at step      250 | time:  0.40s | valid loss  1.81 | valid ppl  6.12 | best ppl  6.12 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 43\n",
      "start to train the model................ 44\n",
      "| epoch  44 step      260 |      2 batches | lr 2.57e-05 | ms/batch 89.80 | loss  3.46 | avg loss  2.96 | ppl 19.30\n",
      "average loss 1.790485421816508\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  26 at step      260 | time:  0.40s | valid loss  1.79 | valid ppl  5.99 | best ppl  5.99 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 45\n",
      "| epoch  45 step      270 |      6 batches | lr 2.29e-05 | ms/batch 266.49 | loss  2.31 | avg loss  2.64 | ppl 13.99\n",
      "average loss 1.7776379982630413\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  27 at step      270 | time:  0.40s | valid loss  1.78 | valid ppl  5.92 | best ppl  5.92 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 46\n",
      "start to train the model................ 47\n",
      "| epoch  47 step      280 |      4 batches | lr 2e-05 | ms/batch 177.82 | loss  2.79 | avg loss  2.78 | ppl 16.10\n",
      "average loss 1.7641421556472778\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  28 at step      280 | time:  0.40s | valid loss  1.76 | valid ppl  5.84 | best ppl  5.84 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 48\n",
      "start to train the model................ 49\n",
      "| epoch  49 step      290 |      2 batches | lr 1.71e-05 | ms/batch 88.73 | loss  3.41 | avg loss  2.91 | ppl 18.31\n",
      "average loss 1.7474995454152424\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  29 at step      290 | time:  0.40s | valid loss  1.75 | valid ppl  5.74 | best ppl  5.74 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "start to train the model................ 50\n",
      "| epoch  50 step      300 |      6 batches | lr 1.43e-05 | ms/batch 267.60 | loss  2.22 | avg loss  2.57 | ppl 13.05\n",
      "average loss 1.7383553981781006\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  30 at step      300 | time:  0.40s | valid loss  1.74 | valid ppl  5.69 | best ppl  5.69 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "if args.do_wandb: \n",
    "    wandb.login()\n",
    "    \n",
    "random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)   \n",
    "\n",
    "run_experiment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
