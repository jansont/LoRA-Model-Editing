{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theojanson/Library/Caches/pypoetry/virtualenvs/lora-XfJfNtEI-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import types\n",
    "import yaml\n",
    "import wandb\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "import itertools\n",
    "import warnings\n",
    "import loralib as lora\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from gpt2_lora.data_utils import FT_Dataset\n",
    "from gpt2_lora.model import GPT2LMModel, GPT2Config, LORAConfig\n",
    "from gpt2_lora.training.train import train_validate\n",
    "from gpt2_lora.training.optimizer import add_optimizer_params_namespace\n",
    "from gpt2_lora.correction_dataset import CorrectionDataset, create_lm_dataset, create_testing_dataset\n",
    "import gpt2_lora.ablations as ablations\n",
    "import gpt2_lora.activation_graft as activation_grafts\n",
    "from gpt2_lora.training.optimizer import (\n",
    "    create_optimizer_scheduler, \n",
    "    add_optimizer_params, \n",
    "    create_adam_optimizer_from_args\n",
    ")\n",
    "from gpt2_lora.exp_utils import create_exp_dir\n",
    "from gpt2_lora.training.evaluate import evaluate\n",
    "from gpt2_lora.utils import set_all_trainable, set_trainable_from_graft, AverageMeter, log_experiment\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc609b04a70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_config():\n",
    "    file = \"../configs/config_lora_compensatory.yaml\"\n",
    "    with open(file, 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    args = types.SimpleNamespace(**config)\n",
    "    args  = add_optimizer_params_namespace(args)\n",
    "    args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    return args\n",
    "\n",
    "args = load_config()\n",
    "random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(n, window_size, step_size):\n",
    "    tensor = torch.arange(n)\n",
    "    windows = [tensor[i:i+window_size] for i in range(0, len(tensor) - window_size + 1, step_size)]\n",
    "    return torch.stack(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lora_configs(layers_to_adapt : list[int], n_layers: int, args : types.SimpleNamespace):\n",
    "    lora_configs = [\n",
    "        {\"attn\" : None, \"mlp\" : None} for _ in range(n_layers)\n",
    "    ]\n",
    "    for layer in layers_to_adapt: \n",
    "        lora_configs[layer][\"attn\"] = LORAConfig(\n",
    "                        layer=layer,\n",
    "                        layer_type=\"attn\",\n",
    "                        adapt_attn_c_attn=args.adapt_attn_c_attn,\n",
    "                        adapt_attn_c_proj=args.adapt_attn_c_proj,\n",
    "                        adapt_mlp_c_fc=False,\n",
    "                        adapt_mlp_c_proj=False,\n",
    "                        lora_dim=args.lora_dim,\n",
    "                        lora_alpha=args.lora_alpha,\n",
    "                        lora_dropout=args.lora_dropout)\n",
    "        \n",
    "        lora_configs[layer][\"mlp\"] = LORAConfig(\n",
    "                        layer=layer,\n",
    "                        layer_type=\"mlp\",\n",
    "                        adapt_attn_c_attn=False,\n",
    "                        adapt_attn_c_proj=False,\n",
    "                        adapt_mlp_c_fc=args.adapt_mlp_c_fc,\n",
    "                        adapt_mlp_c_proj=args.adapt_mlp_c_proj,\n",
    "                        lora_dim=args.lora_dim,\n",
    "                        lora_alpha=args.lora_alpha,\n",
    "                        lora_dropout=args.lora_dropout) \n",
    "    return lora_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_dataset = CorrectionDataset(\"../\" + args.fact_data)\n",
    "correction_dataloader = DataLoader(correction_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in correction_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prompt, subject, target, target_new, neighborhood_prompts,\n",
    "                    same_attribute_prompts, training_prompts) = batch\n",
    "prompt = prompt[0] ; subject = subject[0] ; target = target[0] ; target_new = target_new[0]\n",
    "training_prompts = [prompt[0] for prompt in training_prompts]\n",
    "neighborhood_prompts = [prompt[0] for prompt in neighborhood_prompts]\n",
    "same_attribute_prompts = [prompt[0] for prompt in same_attribute_prompts]\n",
    "\n",
    "if args.model_name == \"gpt2-small\":\n",
    "    hf_model_name = \"gpt2\"\n",
    "    n_layer = 12\n",
    "    config = GPT2Config(\n",
    "        n_embd=768, n_layer=n_layer, n_head=12, \n",
    "    )\n",
    "elif args.model_name == \"gpt2-large\":\n",
    "    hf_model_name = args.model_name\n",
    "    n_layer = 35\n",
    "    config = GPT2Config(\n",
    "        n_embd=1280, n_layer=n_layer, n_head=20, \n",
    "    )\n",
    "else: \n",
    "    raise ValueError(\"model_name not recognized\")\n",
    "\n",
    "windows = generate_sliding_windows(n_layer, args.window_size, args.step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : gpt2_model\n",
      "[\"Danielle Darrieux's native language is English.\", 'English is the first language of Danielle Darrieux.', 'The mother tongue of Danielle Darrieux is the English language.', 'Danielle Darrieux speaks English as her first language.', 'English is the primary language of Danielle Darrieux.', 'Danielle Darrieux grew up speaking English.', 'The language Danielle Darrieux learned first is English.', \"Danielle Darrieux's mother tongue is the English language.\", 'English is the language that Danielle Darrieux speaks fluently.', 'Danielle Darrieux is a native English speaker.', 'The first language Danielle Darrieux learned is English.', \"Danielle Darrieux's native tongue is English.\", 'English is the language Danielle Darrieux was raised speaking.', \"Danielle Darrieux's mother tongue is the English language.\", 'Danielle Darrieux speaks English natively.', 'The language Danielle Darrieux speaks best is English.', 'English is the language Danielle Darrieux is']\n",
      "start to train the model................ 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theojanson/Library/Caches/pypoetry/virtualenvs/lora-XfJfNtEI-py3.9/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to train the model................ 2\n",
      "start to train the model................ 3\n",
      "start to train the model................ 4\n",
      "start to train the model................ 5\n",
      "start to train the model................ 6\n",
      "start to train the model................ 7\n",
      "start to train the model................ 8\n",
      "start to train the model................ 9\n",
      "start to train the model................ 10\n",
      "| epoch  10 step       10 |      1 batches | lr 5e-05 | ms/batch 137.34 | loss  6.11 | avg loss  6.11 | ppl 450.53\n",
      "eval samples: 0 loss: tensor(5.7191) t1_acc: tensor(0.) all_acc: tensor(0.)\n",
      "average loss 5.719144344329834\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step       10 | time:  0.29s | valid loss  5.72 | valid ppl 304.64 | best ppl 304.64 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "eval samples: 0 loss: tensor(5.7191) t1_acc: tensor(0.) all_acc: tensor(0.)\n",
      "average loss 5.719144344329834\n"
     ]
    }
   ],
   "source": [
    "for window in windows: \n",
    "    \n",
    "    lora_configs = generate_lora_configs(layers_to_adapt=window, n_layers=n_layer, args=args)\n",
    "    \n",
    "    lm_net = GPT2LMModel(config, lora_configs)\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(hf_model_name)\n",
    "    state_dict = model.state_dict()\n",
    "    lm_net.load_weight(state_dict)  \n",
    "    \n",
    "    lora.mark_only_lora_as_trainable(lm_net)\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from torch.cuda import amp\n",
    "        except Exception as e:\n",
    "            warnings.warn('Could not import amp, apex may not be installed')\n",
    "\n",
    "    if args.rank == 0:\n",
    "        work_dir = os.getenv('PT_OUTPUT_DIR', 'gpt2_model')\n",
    "        args.logging = create_exp_dir(work_dir)\n",
    "    print(training_prompts)\n",
    "    \n",
    "    dataset = create_lm_dataset(training_prompts, tokenizer, args)\n",
    "    neighbourhood_dataset = create_testing_dataset(neighborhood_prompts, tokenizer, args)\n",
    "    same_attribute_dataset = create_testing_dataset(same_attribute_prompts, tokenizer, args)\n",
    "    \n",
    "    training_prompts, valid_prompts = train_test_split(dataset, test_size=args.test_size, random_state=args.random_seed)\n",
    "    train_data = FT_Dataset(\n",
    "        samples=training_prompts,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    )     \n",
    "    valid_data = FT_Dataset(\n",
    "        samples=valid_prompts,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    )     \n",
    "    neighbourhood_data = FT_Dataset(\n",
    "        samples=neighbourhood_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    )\n",
    "    same_attribute_data = FT_Dataset(\n",
    "        samples=same_attribute_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        max_seq_length=args.seq_len, \n",
    "        joint_lm=args.obj=='jlm'\n",
    "    )        \n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=args.train_batch_size, num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_data, batch_size=args.valid_batch_size, num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=False,\n",
    "    )\n",
    "    neighbourhood_loader = DataLoader(\n",
    "        neighbourhood_data, batch_size=len(neighbourhood_data), num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=False,\n",
    "    )\n",
    "    same_attribute_loader = DataLoader(\n",
    "        same_attribute_data, batch_size=len(same_attribute_data), num_workers=0, \n",
    "        shuffle=False, pin_memory=False, drop_last=False,\n",
    "    )\n",
    "    \n",
    "    optimizer = create_adam_optimizer_from_args(lm_net, args)\n",
    "\n",
    "    if args.max_step is None:\n",
    "        args.max_step = (args.max_epoch * train_data.num_batches) \n",
    "\n",
    "    scheduler = create_optimizer_scheduler(optimizer, args)\n",
    "    if args.fp16:\n",
    "        lm_net, optimizer = amp.initialize(lm_net, optimizer, opt_level=\"O1\")\n",
    "    \n",
    "    try:\n",
    "        train_step = 0\n",
    "        for epoch in itertools.count(start=1):\n",
    "            train_step = train_validate(\n",
    "                lm_net, optimizer, scheduler, train_loader, valid_loader, args, \n",
    "                train_step=train_step, epoch=epoch\n",
    "            )            \n",
    "            if train_step >= args.max_step or (args.max_epoch is not None and epoch >= args.max_epoch):\n",
    "                if args.rank == 0:\n",
    "                    print('-' * 100)\n",
    "                    print('End of training')\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        if args.rank == 0:\n",
    "            print('-' * 100)\n",
    "            print('Exiting from training early')\n",
    "        early_exit = True\n",
    "\n",
    "\n",
    "\n",
    "    testing_loss, testing_ppl, testing_t1, testing_acc = evaluate(lm_net,\n",
    "                                                                    valid_loader,\n",
    "                                                                    args)\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (4134018961.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "lm_net = GPT2LMModel(config, lora_configs)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "state_dict = model.state_dict()\n",
    "lm_net.load_weight(state_dict)   \n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    args.model_name, \n",
    "    hf_model=lm_net,\n",
    "    lora_case=False\n",
    ")\n",
    "\n",
    "logits, cache = model.run_with_cache(\"TEST TOKENS\", return_type=\"logits\")\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hooked_model \u001b[39m=\u001b[39m HookedTransformer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m         args\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m      3\u001b[0m         center_unembed\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \n\u001b[1;32m      4\u001b[0m         center_writing_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,              \u001b[39m# Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m         fold_ln\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,                             \u001b[39m# Whether to  fold in the LayerNorm weights to the subsequent linear layer.\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m         refactor_factored_attn_matrices\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m correction_dataset \u001b[39m=\u001b[39m CorrectionDataset(args\u001b[39m.\u001b[39mfact_data)\n\u001b[1;32m     10\u001b[0m correction_dataloader \u001b[39m=\u001b[39m DataLoader(correction_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/Craft-AI/LoRA/notebooks/../transformer_lens/HookedTransformer.py:928\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[39m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m    926\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(cfg, tokenizer, move_to_device\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 928\u001b[0m model\u001b[39m.\u001b[39;49mload_and_process_state_dict(\n\u001b[1;32m    929\u001b[0m     state_dict,\n\u001b[1;32m    930\u001b[0m     fold_ln\u001b[39m=\u001b[39;49mfold_ln,\n\u001b[1;32m    931\u001b[0m     center_writing_weights\u001b[39m=\u001b[39;49mcenter_writing_weights,\n\u001b[1;32m    932\u001b[0m     center_unembed\u001b[39m=\u001b[39;49mcenter_unembed,\n\u001b[1;32m    933\u001b[0m     fold_value_biases\u001b[39m=\u001b[39;49mfold_value_biases,\n\u001b[1;32m    934\u001b[0m     refactor_factored_attn_matrices\u001b[39m=\u001b[39;49mrefactor_factored_attn_matrices,\n\u001b[1;32m    935\u001b[0m )\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m move_to_device:\n\u001b[1;32m    938\u001b[0m     model\u001b[39m.\u001b[39mmove_model_modules_to_device()\n",
      "File \u001b[0;32m~/Developer/Craft-AI/LoRA/notebooks/../transformer_lens/HookedTransformer.py:1062\u001b[0m, in \u001b[0;36mHookedTransformer.load_and_process_state_dict\u001b[0;34m(self, state_dict, fold_ln, center_writing_weights, center_unembed, fold_value_biases, refactor_factored_attn_matrices)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     state_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfold_value_biases(state_dict)\n\u001b[1;32m   1061\u001b[0m \u001b[39mif\u001b[39;00m refactor_factored_attn_matrices:\n\u001b[0;32m-> 1062\u001b[0m     state_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefactor_factored_attn_matrices(state_dict)\n\u001b[1;32m   1063\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_state_dict(state_dict)\n",
      "File \u001b[0;32m~/Developer/Craft-AI/LoRA/notebooks/../transformer_lens/HookedTransformer.py:1326\u001b[0m, in \u001b[0;36mHookedTransformer.refactor_factored_attn_matrices\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m   1310\u001b[0m W_Q_eff \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m   1311\u001b[0m     [\n\u001b[1;32m   1312\u001b[0m         state_dict[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblocks.\u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m}\u001b[39;00m\u001b[39m.attn.W_Q\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[1;32m   1317\u001b[0m W_K_eff \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m   1318\u001b[0m     [\n\u001b[1;32m   1319\u001b[0m         state_dict[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblocks.\u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m}\u001b[39;00m\u001b[39m.attn.W_K\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1323\u001b[0m )\n\u001b[1;32m   1325\u001b[0m W_Q_eff_even, W_K_eff_even_T \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1326\u001b[0m     FactoredMatrix(W_Q_eff, W_K_eff\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\u001b[39m.\u001b[39;49mmake_even()\u001b[39m.\u001b[39mpair\n\u001b[1;32m   1327\u001b[0m )\n\u001b[1;32m   1328\u001b[0m W_K_eff_even \u001b[39m=\u001b[39m W_K_eff_even_T\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m   1330\u001b[0m state_dict[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblocks.\u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m}\u001b[39;00m\u001b[39m.attn.W_Q\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m W_Q_eff_even[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/Developer/Craft-AI/LoRA/notebooks/../transformer_lens/FactoredMatrix.py:210\u001b[0m, in \u001b[0;36mFactoredMatrix.make_even\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_even\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FactoredMatrix:\n\u001b[1;32m    206\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    Returns the factored form of (U @ S.sqrt().diag(), S.sqrt().diag() @ Vh) where U, S, Vh are the SVD of the matrix. This is an equivalent factorisation, but more even - each half has half the singular values, and orthogonal rows/cols\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[39mreturn\u001b[39;00m FactoredMatrix(\n\u001b[0;32m--> 210\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mU \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mS\u001b[39m.\u001b[39msqrt()[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m, :],\n\u001b[1;32m    211\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mS\u001b[39m.\u001b[39msqrt()[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :, \u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m utils\u001b[39m.\u001b[39mtranspose(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVh),\n\u001b[1;32m    212\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/Craft-AI/LoRA/notebooks/../transformer_lens/FactoredMatrix.py:146\u001b[0m, in \u001b[0;36mFactoredMatrix.U\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    144\u001b[0m \u001b[39m@typeguard_ignore\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mU\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39m*leading_dims ldim mdim\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msvd()[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Developer/Craft-AI/LoRA/notebooks/../transformer_lens/FactoredMatrix.py:135\u001b[0m, in \u001b[0;36mFactoredMatrix.svd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39mEfficient algorithm for finding Singular Value Decomposition, a tuple (U, S, Vh) for matrix M st S is a vector and U, Vh are orthogonal matrices, and U @ S.diag() @ Vh.T == M\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[39m(Note that Vh is given as the transpose of the obvious thing)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m Ua, Sa, Vha \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msvd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA)\n\u001b[0;32m--> 135\u001b[0m Ub, Sb, Vhb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msvd(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mB)\n\u001b[1;32m    136\u001b[0m middle \u001b[39m=\u001b[39m Sa[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :, \u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m utils\u001b[39m.\u001b[39mtranspose(Vha) \u001b[39m@\u001b[39m Ub \u001b[39m*\u001b[39m Sb[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m, :]\n\u001b[1;32m    137\u001b[0m Um, Sm, Vhm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msvd(middle)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    hooked_model = HookedTransformer.from_pretrained(\n",
    "            args.model_name,\n",
    "            center_unembed=True,  \n",
    "            center_writing_weights=True,              # Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \n",
    "            fold_ln=True,                             # Whether to  fold in the LayerNorm weights to the subsequent linear layer.\n",
    "            refactor_factored_attn_matrices=True,\n",
    "        )\n",
    "    \n",
    "    correction_dataset = CorrectionDataset(args.fact_data)\n",
    "    correction_dataloader = DataLoader(correction_dataset, batch_size=1)\n",
    "    early_exit = False\n",
    "        \n",
    "    metrics = [\"loss\", \"ppl\", \"t1\", \"acc\"]\n",
    "    test_sets = [\"testing\", \"neighbourhood\", \"same_attribute\"]\n",
    "    test_metrics = [f\"{t}_{m}\" for m in metrics for t in test_sets]\n",
    "    test_metrics = {test_metric: AverageMeter() for test_metric in test_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lora_configs = [\n",
    "    # LORAConfig(\n",
    "    #         layer=10,\n",
    "    #         adapt_mlp=True,\n",
    "    #         lora_dim=args.lora_dim,\n",
    "    #         lora_alpha=args.lora_alpha,\n",
    "    #         lora_dropout=args.lora_dropout)\n",
    "    ]\n",
    "    \n",
    "if args.model_name == \"gpt2-small\":\n",
    "    hf_model_name = \"gpt2\"\n",
    "    n_layer = 12\n",
    "    config = GPT2Config(\n",
    "        n_embd=768, n_layer=n_layer, n_head=12, \n",
    "    )\n",
    "elif args.model_name == \"gpt2-large\":\n",
    "    hf_model_name = args.model_name\n",
    "    n_layer = 35\n",
    "    config = GPT2Config(\n",
    "        n_embd=5120, n_layer=n_layer, n_head=20, \n",
    "    )\n",
    "else: \n",
    "    raise ValueError(\"model_name not recognized\")\n",
    "\n",
    "lm_net = GPT2LMModel(config, lora_configs)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "state_dict = model.state_dict()\n",
    "lm_net.load_weight(state_dict)   \n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    args.model_name, \n",
    "    hf_model=lm_net,\n",
    "    lora_case=True\n",
    ")\n",
    "\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "\n",
    "#         center_unembed=True,  \n",
    "#         center_writing_weights=True,              # Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \n",
    "#         fold_ln=True,                             # Whether to  fold in the LayerNorm weights to the subsequent linear layer.\n",
    "#         refactor_factored_attn_matrices=True,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora-XfJfNtEI-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
